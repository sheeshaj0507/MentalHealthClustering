{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y99pU-JjzGnd",
    "outputId": "455ad445-61d4-4550-e2be-98bd56ef79e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping abstracts for: Clinical_Depression\n",
      "Scraping abstracts for: Bipolar_Disorder\n",
      "Scraping abstracts for: Anxiety_Disorder\n",
      "Scraping abstracts for: PTSD\n",
      "Scraping abstracts for: Schizophrenia\n",
      "Scraped data saved to 'scraped_pubmed_abstract_1000.csv'.\n",
      "Label\n",
      "Clinical_Depression    200\n",
      "Bipolar_Disorder       200\n",
      "Anxiety_Disorder       200\n",
      "PTSD                   200\n",
      "Schizophrenia          200\n",
      "Name: count, dtype: int64\n",
      "                 Label                                           Abstract\n",
      "0  Clinical_Depression  ObjectiveAdolescence is a formative and turbul...\n",
      "1  Clinical_Depression  Subgroup analysis were performed for year of p...\n",
      "2  Clinical_Depression  Major depressive disorder MDD is considered a ...\n",
      "3  Clinical_Depression  BackgroundBurnout and clinical depression have...\n",
      "4  Clinical_Depression  The interaction of physical and mental vulnera...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Function to extract abstract from a research paper link\n",
    "def extract_abstract(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        abstract = soup.find(\"div\", class_=\"abstract-content selected\")\n",
    "        return abstract.get_text(strip=True) if abstract else None\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "# Function to clean text by removing non-alphanumeric characters except years (19xx, 20xx)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\b(?!19\\d{2}\\b)(?!20\\d{2}\\b)\\d+\\b', '', text)  # Remove numbers except for years\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text\n",
    "\n",
    "# Function to partition text into 150-word segments\n",
    "def partition_text(text, words_per_partition=150):\n",
    "    words = text.split()\n",
    "    partitions = []\n",
    "    \n",
    "    if len(words) < words_per_partition:\n",
    "        return []  # Ignore abstracts that are too short\n",
    "\n",
    "    for i in range(0, len(words) - words_per_partition + 1, words_per_partition // 2):\n",
    "        partition = \" \".join(words[i:i + words_per_partition])\n",
    "        if len(partition.split()) == words_per_partition:\n",
    "            partitions.append(partition)\n",
    "\n",
    "    return partitions\n",
    "\n",
    "# Function to scrape PubMed abstracts for a given query\n",
    "def scrape_pubmed(query, max_results=1500):\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov\"\n",
    "    query_url = f\"{base_url}/?term={query.replace(' ', '+')}&size=200\"\n",
    "    abstracts = []\n",
    "    \n",
    "    total_pages = (max_results // 200) + (1 if max_results % 200 else 0)\n",
    "\n",
    "    for page_num in range(total_pages):\n",
    "        url = f\"{query_url}&page={page_num+1}\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            results = soup.find_all(\"article\", class_=\"full-docsum\")\n",
    "            links = [base_url + article.find(\"a\", class_=\"docsum-title\")[\"href\"] for article in results]\n",
    "\n",
    "            if not links:  # If a page has no results, retry\n",
    "                continue  \n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                for abstract in executor.map(extract_abstract, links):\n",
    "                    if abstract:\n",
    "                        abstracts.append(clean_text(abstract))\n",
    "                    \n",
    "                    if len(abstracts) >= max_results:\n",
    "                        return abstracts  # Stop fetching once limit is reached\n",
    "\n",
    "            time.sleep(2)  # Prevent overwhelming the server\n",
    "        except requests.RequestException:\n",
    "            continue  # Skip if request fails\n",
    "\n",
    "    return abstracts\n",
    "\n",
    "# Define categories and queries\n",
    "categories = {\n",
    "    \"Clinical_Depression\": \"clinical depression\",\n",
    "    \"Bipolar_Disorder\": \"bipolar disorder\",\n",
    "    \"Anxiety_Disorder\": \"anxiety disorder\",\n",
    "    \"PTSD\": \"post-traumatic stress disorder\",\n",
    "    \"Schizophrenia\": \"schizophrenia\"\n",
    "}\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "for label, category in categories.items():\n",
    "    print(f\"Scraping abstracts for: {label}\")\n",
    "\n",
    "    abstracts = scrape_pubmed(category, max_results=1500)  # Fetch more abstracts\n",
    "    \n",
    "    partitions = []\n",
    "    for abstract in abstracts:\n",
    "        partitions.extend(partition_text(abstract, words_per_partition=150))\n",
    "        if len(partitions) >= 200:\n",
    "            break  \n",
    "\n",
    "    # Ensure exactly 200 partitions per category\n",
    "    while len(partitions) < 200:\n",
    "        partitions.append(random.choice(partitions))  # Duplicate random samples if needed\n",
    "\n",
    "    partitions = partitions[:200]\n",
    "\n",
    "    for partition in partitions:\n",
    "        scraped_data.append({\"Label\": label, \"Abstract\": partition})\n",
    "\n",
    "# Save to CSV\n",
    "df_scraped = pd.DataFrame(scraped_data, columns=[\"Label\", \"Abstract\"])\n",
    "df_scraped.to_csv(\"scraped_pubmed_abstract_1000.csv\", index=False)\n",
    "\n",
    "print(\"Scraped data saved to 'scraped_pubmed_abstract_1000.csv'.\")\n",
    "print(df_scraped['Label'].value_counts())  # Verify category balance\n",
    "print(df_scraped.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Data_Science_NLP)",
   "language": "python",
   "name": "data_science_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
