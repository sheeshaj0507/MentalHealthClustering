Group: 09
Assignment 2
Team: Sheryl, Lenish, Lakshika
Date: February 27, 2025

---------------------------------ABOUT---------------------------------

Welcome! In this program, we cluster research paper abstracts from PubMed medical literature database under five mental health disorder categories, namely clinical depression, bipolar disorder, anxiety disorder, post-traumatic stress disorder and schizophrenia. 

As it is a text clustering exercise, we preprocess and cleanse the abstracts, then feature engineer it using BERT+Glove embedding techniques to be able to use in clustering algorithms. 
We implement the models using K-Means clustering, Agglomerative clustering and GMM models. 
Thereafter, we evaluate the models using Kappa score and sillhouette values and perform evaluations. 
Moreover we use accuracy score, classification report, confusion matrix and perform error analysis, and additional visualizations. Refer to the report for more details.

Following files are part of the program:
- G9_A2_readme
- G9_A2_abstract_extraction.ipynb (generates the dataset)
- scraped_pubmed_abstract_150.csv (dataset)
- G9_A2_TextClustering_final.ipynb 
- GloVe file - glove.6B100d.txt
- G9_A2_Clustering_Project (Report)
- G9_A2_MentalHealthAbstractsProject_Clustering (Presentation)

---------------------------------PREREQUISITES---------------------------------

Install the following libraries (pip install...):
- pandas
- numpy
- re
- string
- nltk
- seaborn
- sklearn
- matplotlib
- gensim.models
- collections
- scipy


Download the necessary NLTK resources:
- nltk.download('punkt')
- nltk.download('stopwords')
- nltk.download('wordnet')
- nltk.download('punkt_tab')

Environment (recommended): 
- Google Collab (Python 3)

---------------------------------DATASET---------------------------------

We perform a web scraping of the public PubMed database under five mental health disorders (i.e, categories: anxiety disorder, bipolar disorder, clinical depression, post-traumatic stress disorder and schizophrenia) and generate a file with labels and abstracts, on which we perform the data preparation and transformation. We share this initially cleansed file as part of the program so that the rest of the code run efficiently.

The dataset has 150 words per partition. The dataset has 1000 abstracts with the corresponding true category (i.e., label). Each category has 200 partitions of abstracts, with the corresponding number of words. 

These two files are:

scraped_pubmed_abstract_150.csv


---------------------INSTRUCTIONS TO RUN THE PROGRAM---------------------

1) Install the listed libraries as prerequisites. 
2) Run the 'abstract_extraction.ipynb' file separately, to generate the dataset. This is omitted from the main program code as it takes a few minutes to generate the dataset. Hence, we import the dataset file when running the main program.
3) Import the 'G9_Assignment1_Text_Clustering.ipynb' file into your environment. 
4) Import the abstract file (dataset) and the GloVe file into Goolge Collab or your Jupyter Notebook location.
5) Run each code block with the 'scraped_pubmed_abstract_150.csv' dataset in order and view the results. **

** Alternatively, you can view the code and results which are available in each file, without running it again.

---------------------------------THE END---------------------------------

